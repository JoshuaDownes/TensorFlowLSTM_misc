{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bigram LSTM\n",
    "\n",
    "Model for predicting text sequences a bigram at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Modules\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and Extracting Data\n",
    "\n",
    "Bigrams are generated by splitting the text into non-overlapping pairs of characters.\n",
    "e.g. 'abc123' -> ['ab','c1','23']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified\n",
      "Total characters: 100000000\n",
      "Total bigrams: 50000000\n",
      "Sample text:  anarchism originated as a term of abuse first use\n",
      "Bigram text: [' a', 'na', 'rc', 'hi', 'sm', ' o', 'ri', 'gi', 'na', 'te']\n"
     ]
    }
   ],
   "source": [
    "def get_text8(expected_bytes):\n",
    "    filename = \"text8.zip\"\n",
    "    url = \"http://mattmahoney.net/dc/\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    if os.stat(filename).st_size == expected_bytes:\n",
    "        print('Found and verified')\n",
    "    else:\n",
    "        raise Exception('Failed to verify. Try accessing through browser.')\n",
    "    return filename\n",
    "\n",
    "def read_text(filename):\n",
    "    f = zipfile.ZipFile(filename)\n",
    "    for name in f.namelist():\n",
    "        return tf.compat.as_str(f.read(name))\n",
    "    f.close()\n",
    "    return\n",
    "   \n",
    "expected_bytes = 31344016\n",
    "filename = get_text8(expected_bytes)\n",
    "text = read_text(filename)\n",
    "bigrams = [text[2*i:2*i+2] for i in range(len(text)/2)]\n",
    "\n",
    "print('Total characters: %d' % len(text))\n",
    "print('Total bigrams: %d' % len(bigrams))\n",
    "print('Sample text: %s' % text[:50])\n",
    "print('Bigram text: %s' % bigrams[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Lookup for Inputs\n",
    "Train a bigram-embedding via word2vec skip-gram model. \n",
    "\n",
    "Our embedding will map each bigram to a vector of form [x1,x2,...,xn]\n",
    "\n",
    "Word2vec skip-gram model trains an embedding based on the context of the word; words which appear in similar contexts have similar embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most popular bigrams:  [['NA', 230382], ('e ', 1843425), (' t', 1224131), ('s ', 1111188), ('th', 990343)]\n",
      "Reverse Dictionary:   a\n",
      "Sample Data:  [5, 96, 220, 75, 267, 10, 56, 196, 96, 29]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 400\n",
    "\n",
    "def build_dataset(bigrams):\n",
    "    \n",
    "    # Find counts of all bigrams\n",
    "    bigram_counts = [['NA',-1]]\n",
    "    most_common = collections.Counter(bigrams).most_common(vocabulary_size - 1)\n",
    "    bigram_counts.extend(most_common)\n",
    "    \n",
    "    # Dictionary which maps a bigram to its popularity, with 0 being the most popular\n",
    "    dictionary = {}\n",
    "    for bigram, _ in bigram_counts:\n",
    "        dictionary[bigram] = len(dictionary)\n",
    "        \n",
    "    # Reverse dictionary which maps popularity to a bigram\n",
    "    reverse_dictionary = dict(zip(dictionary.values(),dictionary.keys()))\n",
    "    \n",
    "    # Data is an array storing each bigram's key in our reverse_dictionary\n",
    "    # i.e. bigram[i] = reverse_dictionary[data[i]]\n",
    "    data = []\n",
    "    NA_count = 0\n",
    "    for bigram in bigrams:\n",
    "        if bigram in dictionary:\n",
    "            index = dictionary[bigram]\n",
    "        else:\n",
    "            index = 0\n",
    "            NA_count += 1\n",
    "        data.append(index)\n",
    "    bigram_counts[0][1] = NA_count\n",
    "    \n",
    "    return data, bigram_counts, dictionary, reverse_dictionary\n",
    "\n",
    "data, bigram_counts, dictionary, reverse_dictionary = build_dataset(bigrams)\n",
    "print('Most popular bigrams: ',bigram_counts[:5])\n",
    "print('Reverse Dictionary: ',reverse_dictionary[5])\n",
    "print('Sample Data: ', data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: ['rc', 'rc', 'rc', 'rc', 'hi', 'hi', 'hi', 'hi']\n",
      "Labels: ['na', 'hi', 'sm', ' a', ' o', 'rc', 'sm', 'na']\n"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "\n",
    "def generate_batch(batch_size,num_skips,skip_window):\n",
    "    \n",
    "    # Setup\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    \n",
    "    # Batch and labels\n",
    "    batch = np.ndarray(shape=(batch_size),dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size,1),dtype=np.int32)\n",
    "    \n",
    "    # Create a buffer and add first valid span to it.\n",
    "    # We use a deque with (maxlen=span). This way, when we attempt to add more elements \n",
    "    # than fit in a bigram's span, the front element will automatically be popped\n",
    "    span = 2 * skip_window + 1\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "        \n",
    "    # Fill batch and labels \n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window \n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            # Pick a random bigram in the span, add it to 'used' list\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0,span - 1) \n",
    "            targets_to_avoid.append(target)\n",
    "            # Add the target and the selected context-bigram to batch and labels\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "        # Adjust the buffer to the next bigram's span\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    \n",
    "    return batch, labels\n",
    "\n",
    "data_index = 0\n",
    "batch, labels = generate_batch(batch_size=8,num_skips=4,skip_window=2)\n",
    "print(\"Batch:\",[reverse_dictionary[bi] for bi in batch])\n",
    "print(\"Labels:\",[reverse_dictionary[li] for li in labels.reshape(-1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hyper params\n",
    "batch_size = 128\n",
    "embedding_size = 128  # Size of vector we map bigrams to\n",
    "skip_window = 2       # Bigrams we consider left and right\n",
    "num_skips = 4         # Times we reuse an input to generate a label\n",
    "\n",
    "# Validation set\n",
    "valid_size = 16       # Random set of words to evaluate similarity on\n",
    "valid_window = 100    # Only pick dev samples in head of distribution\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "num_sampled = 64      # Negative examples to sample\n",
    "\n",
    "# Build graph\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "    \n",
    "    # Input data\n",
    "    train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    \n",
    "    # Variables\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    softmax_weights = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, embedding_size], \n",
    "                            stddev = 1.0 / math.sqrt(embedding_size)))\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Model\n",
    "    # Look up embedding\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "    # Compute softmax\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.sampled_softmax_loss(softmax_weights,softmax_biases,embed,\n",
    "                                   train_labels, num_sampled, vocabulary_size))\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "    \n",
    "    # Compute minibatch-to-all embedding similarity\n",
    "    # Cosine distance\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(\n",
    "        normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run TF Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Avg loss step 0: 4.815992\n",
      "Avg loss step 2000: 3.839729\n",
      "Avg loss step 4000: 3.701947\n",
      "Avg loss step 6000: 3.651694\n",
      "Avg loss step 8000: 3.610718\n",
      "Avg loss step 10000: 3.627809\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "\n",
    "# tensorflow session\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_data, batch_labels = generate_batch(batch_size,num_skips,skip_window)\n",
    "        feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += l\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss = average_loss / 2000\n",
    "            print(\"Avg loss step %d: %f\" % (step,average_loss))\n",
    "            average_loss = 0\n",
    "    \n",
    "    final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
